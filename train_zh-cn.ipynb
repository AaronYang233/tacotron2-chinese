{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@title 安装依赖\n",
    "# !pip install tensorflow\n",
    "%pip install --upgrade pip\n",
    "%pip install torch\n",
    "%pip uninstall -y tensorflow numba\n",
    "%pip install inflect==0.2.5\n",
    "%pip install librosa==0.6.0\n",
    "%pip install Unidecode==1.0.22\n",
    "%pip install pillow\n",
    "%pip install janome==0.4.2\n",
    "%pip install denoiser==0.1.0\n",
    "%pip install librosa==0.8.0\n",
    "%pip install pysoundfile==0.9.0.post1\n",
    "%pip install unidecode==1.3.4\n",
    "%pip install numpy==1.20\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "%pip install pyopenjtalk\n",
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 训练模型的代码\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from numpy import finfo\n",
    "\n",
    "import torch\n",
    "from distributed import apply_gradient_allreduce\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Tacotron2\n",
    "from data_utils import TextMelLoader, TextMelCollate\n",
    "from loss_function import Tacotron2Loss\n",
    "from logger import Tacotron2Logger\n",
    "from hparams import create_hparams\n",
    " \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import layers\n",
    "from utils import load_wav_to_torch, load_filepaths_and_text\n",
    "from text import text_to_sequence\n",
    "from math import e\n",
    "#from tqdm import tqdm # Terminal\n",
    "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
    "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
    "from distutils.dir_util import copy_tree\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def download_from_google_drive(file_id, file_name):\n",
    "  # download a file from the Google Drive link\n",
    "  !rm -f ./cookie\n",
    "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
    "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
    "  confirm_text = confirm_text[0]\n",
    "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
    "\n",
    "def create_mels():\n",
    "    print(\"Generating Mels\")\n",
    "    stft = layers.TacotronSTFT(\n",
    "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                hparams.mel_fmax)\n",
    "    \n",
    "    def save_mel(filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n",
    "                sampling_rate, stft.sampling_rate))\n",
    "        audio_norm = audio / hparams.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
    "        np.save(filename.replace('.wav', ''), melspec)\n",
    "\n",
    "    import glob\n",
    "    wavs = glob.glob('OPENCPOP/wavs/*.wav')\n",
    "    for i in tqdm(wavs):\n",
    "        save_mel(i)\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt\n",
    "\n",
    "\n",
    "def init_distributed(hparams, n_gpus, rank, group_name):\n",
    "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
    "    print(\"Initializing Distributed\")\n",
    "\n",
    "    # Set cuda device so everything is done on the right GPU.\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "    # Initialize distributed communication\n",
    "    dist.init_process_group(\n",
    "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
    "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
    "\n",
    "    print(\"Done initializing distributed\")\n",
    "\n",
    "\n",
    "def prepare_dataloaders(hparams):\n",
    "    # Get data, data loaders and collate function ready\n",
    "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
    "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
    "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=hparams.batch_size, pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2(hparams).cuda()\n",
    "    if hparams.fp16_run:\n",
    "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
    "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
    "        iteration, filepath))\n",
    "    try:\n",
    "        torch.save({'iteration': iteration,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt received while saving, waiting for save to complete.\")\n",
    "        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def plot_alignment(alignment, info=None):\n",
    "    %matplotlib inline\n",
    "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
    "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
    "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            if distributed_run:\n",
    "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "        if hparams.show_alignments:\n",
    "            %matplotlib inline\n",
    "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
    "            idx = random.randint(0, alignments.size(0) - 1)\n",
    "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
    "\n",
    "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
    "          rank, group_name, hparams, log_directory2):\n",
    "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    output_directory (string): directory to save checkpoints\n",
    "    log_directory (string) directory to save tensorboard logs\n",
    "    checkpoint_path(string): checkpoint path\n",
    "    n_gpus (int): number of gpus\n",
    "    rank (int): rank of current gpu\n",
    "    hparams (object): comma separated list of \"name=value\" pairs.\n",
    "    \"\"\"\n",
    "    if hparams.distributed_run:\n",
    "        init_distributed(hparams, n_gpus, rank, group_name)\n",
    "\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "    model = load_model(hparams)\n",
    "    learning_rate = hparams.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                 weight_decay=hparams.weight_decay)\n",
    "\n",
    "    if hparams.fp16_run:\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level='O2')\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    logger = prepare_directories_and_logger(\n",
    "        output_directory, log_directory, rank)\n",
    "\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "\n",
    "    # Load checkpoint if one exists\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams.ignore_layers)\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams.use_saved_learning_rate:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1  # next iteration is iteration + 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "    else:\n",
    "      os.path.isfile(\"tacotron2_statedict.pt\")\n",
    "      model = warm_start_model(\"tacotron2_statedict.pt\", model, hparams.ignore_layers)\n",
    "      # download LJSpeech pretrained model if no checkpoint already exists\n",
    "    \n",
    "    start_eposh = time.perf_counter()\n",
    "    learning_rate = 0.0\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
    "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
    "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            start = time.perf_counter()\n",
    "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
    "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
    "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            if hparams.distributed_run:\n",
    "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            if hparams.fp16_run:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if hparams.fp16_run:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
    "                is_overflow = math.isnan(grad_norm)\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), hparams.grad_clip_thresh)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
    "\n",
    "            iteration += 1\n",
    "        validate(model, criterion, valset, iteration,\n",
    "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
    "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
    "        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
    "        if log_directory2 != None:\n",
    "            copy_tree(log_directory, log_directory2)\n",
    "def check_dataset(hparams):\n",
    "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
    "    import os\n",
    "    import numpy as np\n",
    "    def check_arr(filelist_arr):\n",
    "        for i, file in enumerate(filelist_arr):\n",
    "            if len(file) > 2:\n",
    "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
    "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
    "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
    "            else:\n",
    "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
    "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
    "            if (not os.path.exists(file[0])):\n",
    "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
    "            if len(file[1]) < 3:\n",
    "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
    "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
    "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
    "            mel_length = 1\n",
    "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
    "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
    "                mel_length = melspec.shape[1]\n",
    "            if mel_length == 0:\n",
    "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
    "    print(\"Checking Training Files\")\n",
    "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
    "    check_arr(audiopaths_and_text)\n",
    "    print(\"Checking Validation Files\")\n",
    "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
    "    check_arr(audiopaths_and_text)\n",
    "    print(\"Finished Checking\")\n",
    "\n",
    "warm_start=False #sorry about that\n",
    "n_gpus=1\n",
    "rank=0\n",
    "group_name=None\n",
    "\n",
    "# ---- 这是定义的默认参数，可以不用管 ----\n",
    "hparams = create_hparams()\n",
    "model_filename = 'IM'\n",
    "hparams.training_files = \"filelists/cpop_audio_text_train_filelist_new.txt\"\n",
    "hparams.validation_files = \"filelists/cpop_audio_text_val_filelist_new.txt\"\n",
    "#hparams.use_mmi=True,          # not used in this notebook\n",
    "#hparams.use_gaf=True,          # not used in this notebook\n",
    "#hparams.max_gaf=0.5,           # not used in this notebook\n",
    "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
    "hparams.p_attention_dropout=0.1\n",
    "hparams.p_decoder_dropout=0.1\n",
    "hparams.decay_start = 15000\n",
    "hparams.A_ = 5e-4\n",
    "hparams.B_ = 8000\n",
    "hparams.C_ = 0\n",
    "hparams.min_learning_rate = 1e-5\n",
    "generate_mels = True\n",
    "hparams.show_alignments = True\n",
    "alignment_graph_height = 600\n",
    "alignment_graph_width = 1000\n",
    "hparams.batch_size = 32\n",
    "hparams.load_mel_from_disk = True\n",
    "hparams.ignore_layers = []\n",
    "hparams.epochs = 10000\n",
    "\n",
    "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
    "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
    "output_directory = 'sys_outdir' # Location to save Checkpoints\n",
    "log_directory = 'tacotron2_logs' # Location to save Log files locally\n",
    "log_directory2 = 'tacotron2_logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)e\n",
    "checkpoint_path = output_directory+(r'/')+model_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型参数\n",
    "\n",
    "#@title 设置参数\n",
    "\n",
    "#@markdown **这两个参数是最重要的。**\n",
    "\n",
    "#@markdown 这个参数控制模型训练得多快。**不要设置太大，否则显卡会炸。**如果数据集比较大，设置在30左右比较好。\n",
    "\n",
    "#@markdown 如果数据集里音频文件的数量和这个参数差不多，训练会失败。\n",
    "\n",
    "hparams.batch_size = 8 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown 这个参数控制训练的次数\n",
    "hparams.epochs = 1000 #@param {type:\"integer\"}\n",
    "\n",
    "#The rest aren't that important\n",
    "hparams.p_attention_dropout=0.1\n",
    "hparams.p_decoder_dropout=0.1\n",
    "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
    "hparams.A_ = 5e-4                   # Start/Max Learning Rate\n",
    "hparams.B_ = 8000                   # Decay Rate\n",
    "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
    "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
    "generate_mels = True # Don't change\n",
    "hparams.show_alignments = True\n",
    "alignment_graph_height = 600\n",
    "alignment_graph_width = 1000\n",
    "hparams.load_mel_from_disk = True\n",
    "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
    "\n",
    "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
    "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
    "output_directory = 'sys_outdir' # Location to save Checkpoints\n",
    "log_directory = 'tacotron2_logs' # Location to save Log files locally\n",
    "log_directory2 = 'tacotron2_logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)e\n",
    "checkpoint_path = output_directory+(r'/')+model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filelists/cpop_audio_text_test_filelist_new.txt\n",
      "filelists/cpop_audio_text_test_filelist_new.txt\n",
      "['japanese_phrase_cleaners']\n"
     ]
    }
   ],
   "source": [
    "# 数据集预处理\n",
    "# 如果要求不高，两个列表用同一个文件即可\n",
    "\n",
    "# 验证集\n",
    "training_files_name = \"cpop_audio_text_test_filelist_new.txt\" #@param {type:\"string\"}\n",
    "# 文件列表\n",
    "validation_files_name = \"cpop_audio_text_test_filelist_new.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# 预处理文本的cleaner\n",
    "hparams_prefix = \"filelists/\"\n",
    "\n",
    "text_cleaner='japanese_phrase_cleaners' #@param {type:\"string\"}\n",
    "text_cleaners=[text_cleaner]\n",
    "\n",
    "#@markdown ### 各种cleaner的效果示例\n",
    "#@markdown ### 1. 'japanese_cleaners'\n",
    "#@markdown #### 处理前\n",
    "#@markdown 你好\n",
    "#@markdown #### 处理后\n",
    "#@markdown nihao\n",
    "\n",
    "training_files = hparams_prefix + training_files_name\n",
    "validation_files = hparams_prefix + validation_files_name\n",
    "\n",
    "hparams.training_files = training_files\n",
    "hparams.validation_files = validation_files\n",
    "hparams.text_cleaners = text_cleaners\n",
    "\n",
    "print(hparams.training_files)\n",
    "print(hparams.validation_files)\n",
    "print(hparams.text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Replace .wav with .npy in filelists ----\n",
    "# !sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
    "# ---- Replace .wav with .npy in filelists ----\n",
    "# if generate_mels:\n",
    "#     create_mels()\n",
    "\n",
    "#@title 检查数据集\n",
    "#@markdown 没有error就算成功\n",
    "# check_dataset(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 Run: False\n",
      "Dynamic Loss Scaling: True\n",
      "Distributed Run: False\n",
      "cuDNN Enabled: True\n",
      "cuDNN Benchmark: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001A806F4F9D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anconda\\envs\\ysf\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\anconda\\envs\\ysf\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuDNN Enabled:\u001b[39m\u001b[38;5;124m'\u001b[39m, hparams\u001b[38;5;241m.\u001b[39mcudnn_enabled)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuDNN Benchmark:\u001b[39m\u001b[38;5;124m'\u001b[39m, hparams\u001b[38;5;241m.\u001b[39mcudnn_benchmark)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_directory2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 240\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)\u001b[0m\n\u001b[0;32m    237\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(hparams\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m    238\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(hparams\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m--> 240\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m    242\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    243\u001b[0m                              weight_decay\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mweight_decay)\n",
      "Cell \u001b[1;32mIn[14], line 122\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(hparams):\n\u001b[1;32m--> 122\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTacotron2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hparams\u001b[38;5;241m.\u001b[39mfp16_run:\n\u001b[0;32m    124\u001b[0m         model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mattention_layer\u001b[38;5;241m.\u001b[39mscore_mask_value \u001b[38;5;241m=\u001b[39m finfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmin\n",
      "File \u001b[1;32md:\\anconda\\envs\\ysf\\lib\\site-packages\\torch\\nn\\modules\\module.py:916\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anconda\\envs\\ysf\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anconda\\envs\\ysf\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32md:\\anconda\\envs\\ysf\\lib\\site-packages\\torch\\nn\\modules\\module.py:916\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\anconda\\envs\\ysf\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#@title 开始训练\n",
    "#@markdown  Validation loss 越小，拟合效果可能越好\n",
    "print('FP16 Run:', hparams.fp16_run)\n",
    "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
    "print('Distributed Run:', hparams.distributed_run)\n",
    "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
    "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
    "train(output_directory, log_directory, checkpoint_path,\n",
    "      warm_start, n_gpus, rank, group_name, hparams, log_directory2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
